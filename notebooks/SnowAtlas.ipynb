{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We need to convert the file type and reproject our Snow Water Equivalent data into the format accepted by Google Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists, join\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import re\n",
    "import rioxarray\n",
    "import subprocess\n",
    "import urllib\n",
    "import xarray as xr\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE Grids Map Projection to WGS84\n",
    "\n",
    "The GlobSnow data product is produced by the European Space Agency as a HDF4 file. Plotting the SWE from the original file using xarray shows a Northern Hemisphere, Lambert Azimuthal projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Nio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d26f6404abe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/01/GlobSnow_SWE_L3B_monthly_199701_v2.0.hdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_cf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pynio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswe_average\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs, use_cftime)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pynio'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             store = backends.NioDataStore(\n\u001b[0;32m--> 431\u001b[0;31m                 filename_or_obj, lock=lock, **backend_kwargs)\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pseudonetcdf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             store = backends.PseudoNetCDFDataStore.open(\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/backends/pynio_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, lock, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mNio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPYNIO_LOCK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Nio'"
     ]
    }
   ],
   "source": [
    "f = \"../data/01/GlobSnow_SWE_L3B_monthly_199701_v2.0.hdf\"\n",
    "data = xr.open_dataset(f, decode_cf=False, engine='pynio')\n",
    "data.swe_average.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to reproject this to WGS84. Unfortunately, if we look at the xarray dataset we see there are no latitude, longitude data variables. The metadata concerning the projection is missing. Without this projection information, GDAL does not know how to map the pixel coordinates to latitude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:      (fakeDim0: 721, fakeDim1: 721, fakeDim2: 721, fakeDim3: 721)\n",
      "Dimensions without coordinates: fakeDim0, fakeDim1, fakeDim2, fakeDim3\n",
      "Data variables:\n",
      "    swe_average  (fakeDim0, fakeDim1) float32 ...\n",
      "    swe_maximum  (fakeDim2, fakeDim3) float32 ...\n",
      "Attributes:\n",
      "    Data content, field 1:        Monthly mean Snow Water Equivalent (mm)\n",
      "    Data content, field 2:        Monthly maximum Snow Water Equivalent (mm)\n",
      "    Sensor :                      SSM/I\n",
      "    Data Date :                   yyyymmdd\n",
      "    Processing Date:              yyyymmdd\n",
      "    Coordinate system :           Equal-Area Scalable Earth Grid (EASE-Grid) ...\n",
      "    Latitude range:               35N - 85N\n",
      "    Longitude range:              180W - 180E\n",
      "    Spatial Resolution :          25 X 25 sq.km\n",
      "    Processing software name:     FMI assimilation algorithm (Pullianen 2006)\n",
      "    Processing software version:  v 2.0\n",
      "    Processing Organisation:      Finnish Meteorological Institute\n",
      "    Landmask :                     GLC-2000 derived land classification mask\n",
      "    Landmask version:             v 2.0\n",
      "    Mountain mask :               ETOPO-5 derived mountain mask\n",
      "    Mountain mask version :       v 2.0\n",
      "    forest mask name :            GLC-2000 derived forest mask\n",
      "    forest mask version :         v 2.0\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically obtain projection data\n",
    "\n",
    "The parameters of the grid can be found at the [National Snow & Ice Data Center](https://nsidc.org/ease/ease-grid-projection-gt). We wrote a class to obtain the projection data from the data provided on the NSIDC website. The EPSG and map coordinates of pixel corners must be manually provided to the GDAL translate utility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE_Parameters(object):\n",
    "    \n",
    "    def __init__(self, version=1, hemisphere='Northern'):\n",
    "\n",
    "        '''\n",
    "        Retrieve CRS information from URL.\n",
    "        \n",
    "        :param version: The version number (1 or 2) of the grid\n",
    "        :param hemisphere: The hemisphere of interest (Northern or Southern)\n",
    "        '''\n",
    "        # Set the grid name using the supplied version number\n",
    "        self.version = version\n",
    "        grid_name = {\n",
    "            1: 'EASE-Grid',\n",
    "            2: 'EASE-Grid 2.0'\n",
    "        }\n",
    "        # Validate user input\n",
    "        try:\n",
    "            self.grid = grid_name[self.version]\n",
    "        except KeyError:\n",
    "            raise UserWarning('You must choose between version 1 and version 2.')\n",
    "        \n",
    "        # Validate the hemisphere input\n",
    "        self.hemisphere = hemisphere\n",
    "        if self.hemisphere.lower() not in ['northern', 'southern']:\n",
    "            raise UserWarning('You must chose between the Northern and Southern hemisphere.')\n",
    " \n",
    "        self.url = \"https://nsidc.org/ease/ease-grid-projection-gt\"\n",
    "        self._table_number = -2\n",
    "        \n",
    "    @property\n",
    "    def table_number(self):\n",
    "        return self._table_number\n",
    "    \n",
    "    @table_number.setter\n",
    "    def table_number(self, n):\n",
    "        self._table_number = n\n",
    "\n",
    "    @property\n",
    "    def table(self):\n",
    "        '''\n",
    "        Return the dataframe for the grid and resolution of interest.\n",
    "        Rather than pass resolution as a parameter, select the table number from\n",
    "        \n",
    "        '''\n",
    "        tables = pd.read_html(self.url)\n",
    "        num_tables = len(tables) -1\n",
    "        if abs(self._table_number) > num_tables:\n",
    "            raise UserWarning(f'There are only {num_tables} tables, please select a valid table number.')\n",
    "        \n",
    "        df = tables[self._table_number]\n",
    "        # The Grid Name column name varies over tables, use column number\n",
    "        return df[(df[df.columns[0]]==self.grid) & (df['Projection'].str.contains(self.hemisphere, case=False))]\n",
    "    \n",
    "    @property\n",
    "    def epsg(self):\n",
    "        '''\n",
    "        Return the EPSG code corresponding to the CRS. \n",
    "        '''\n",
    "        proj = self.table['Projection'].item()\n",
    "        pattern = re.compile('EPSG: \\d{4}')\n",
    "        epsg = re.search(pattern, proj)[0]\n",
    "        return epsg.replace(' ', '')\n",
    "    \n",
    "    @property\n",
    "    def num_cols(self):\n",
    "        '''\n",
    "        Return the number of columns in the grid.\n",
    "        '''\n",
    "        return self.table['Number of Columns'].item()\n",
    "\n",
    "    @property\n",
    "    def num_rows(self):\n",
    "        '''\n",
    "        Return the number of rows in teh grid.\n",
    "        '''\n",
    "        return self.table['Number of Rows'].item()\n",
    "    \n",
    "    @property\n",
    "    def grid_size(self):\n",
    "        '''\n",
    "        Grid cell area is reported as a string {number} {unit} x {number} {unit}.\n",
    "        Parse the string and take the first number as the size\n",
    "        '''\n",
    "        return float(self.table['Grid Cell Area'].item().split(' ')[0].replace(',',''))\n",
    "    \n",
    "    # TODO: Debug key error \n",
    "    @property\n",
    "    def ulx(self):\n",
    "        try:\n",
    "            return self.table['x-axis map coordinate of the outer edge of the upper-left pixel'].item()\n",
    "        except KeyError:\n",
    "            return self.table[self.table.columns[-2]].item()\n",
    "                \n",
    "    @property\n",
    "    def uly(self):\n",
    "        try:\n",
    "            return self.table['y-axis map coordinate of the outer edge of the upper-left pixel'].item()\n",
    "        except KeyError:\n",
    "            return self.table[self.table.columns[-1]].item()\n",
    "    \n",
    "    @property\n",
    "    def lrx(self):\n",
    "        '''\n",
    "        Return the lower right x coord by multiplying the grid size by the number of columns.\n",
    "        '''\n",
    "        return self.ulx + self.grid_size*self.num_cols\n",
    "    \n",
    "    @property \n",
    "    def lry(self):\n",
    "        '''\n",
    "        Return the lower right x coord by multiplying the grid size by the number of rows.\n",
    "        '''\n",
    "        return self.uly - self.grid_size*self.num_rows\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HDF file to TIF file\n",
    "When GDAL is given the projection information, it can be used to convert to a GeoTiff file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_globsnow(directory):\n",
    "    '''\n",
    "    Download all available data.\n",
    "    '''\n",
    "    for year in range(1997, 2017):\n",
    "        for month in range(1, 13):\n",
    "            mon = \"{0:0=2d}\".format(month)\n",
    "            filename = f\"GlobSnow_SWE_L3B_monthly_{year}{mon}_v2.0.hdf\"\n",
    "            folder = join(directory, mon)\n",
    "            if not exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            if not exists(join(folder, filename)):\n",
    "                url = f\"http://www.globsnow.info/swe/archive_v2.0/{year}/L3B_monthly_SWE_HDF/{filename}\"\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, join(folder, filename))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# File conversion function \n",
    "def translate_globsnow(directory, month):\n",
    "    '''\n",
    "    Add meta data to the raw file and convert to gdal virtual format.\n",
    "    '''\n",
    "    # Obtain CRS information\n",
    "    param = EASE_Parameters()\n",
    "    epsg = param.epsg\n",
    "    ulx, uly, lrx, lry = param.ulx, param.uly, param.lrx, param.lry\n",
    "    \n",
    "    # Find files that match this pattern\n",
    "    pattern = re.compile(\"(GlobSnow_SWE_L3B_monthly_(\\d{4})(\\d{2})_v2.0.hdf)\")\n",
    "    folder = join(directory, month)\n",
    "    for f in glob(join(folder, '*.hdf')):\n",
    "        match = re.search(pattern, f)\n",
    "        if match is not None:\n",
    "            # Create output filename from input filename\n",
    "            input_file, year, month = match.groups()\n",
    "            output_file = f\"GlobSnow_SWE_Average_{year}_{month}.vrt\"\n",
    "            bash_cmd = (\n",
    "                f\"gdal_translate -of VRT -a_nodata -1 \"\n",
    "                f\"-a_srs {epsg} -a_ullr {ulx} {uly} {lrx} {lry} \"\n",
    "                f\"HDF4_SDS:UNKNOWN:\\\"{input_file}\\\":0 {output_file}\"\n",
    "            )\n",
    "            subprocess.Popen(bash_cmd, cwd=folder, shell=True, executable='/bin/bash')\n",
    "\n",
    "def convert_globsnow(directory, month):\n",
    "    '''\n",
    "    Convert .vrt to a .tif file.\n",
    "    '''\n",
    "    pattern = re.compile(\"GlobSnow_SWE_Average_\\d{4}_\\d{2}.vrt\")\n",
    "    folder = join(directory, month)\n",
    "    for f in glob(join(folder, '*.vrt')):\n",
    "        match = re.search(pattern, f)\n",
    "        if match is not None:\n",
    "            input_file = match[0]\n",
    "            output_file = input_file.replace('.vrt', '.tif')\n",
    "            bash_cmd = (\n",
    "                f\"gdalwarp -of GTiff \"\n",
    "                f\"-t_srs EPSG:4326 -r cubic \"\n",
    "                f\"{input_file} {output_file}\"\n",
    "            )\n",
    "            subprocess.Popen(bash_cmd, cwd=folder, shell=True, executable='/bin/bash')\n",
    "     \n",
    "    \n",
    "def plot(filename):\n",
    "    '''\n",
    "    Plot image.\n",
    "    '''\n",
    "    im = Image.open(filename)\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replotting the data we can see it has been reprojected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code \n",
    "download_globsnow(\"../data\")\n",
    "translate_globsnow(\"../data\", '01')\n",
    "# convert_globsnow(\"../data\", '01')\n",
    "# plot(\"../data/GlobSnow_SWE_Average_2016_01.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
