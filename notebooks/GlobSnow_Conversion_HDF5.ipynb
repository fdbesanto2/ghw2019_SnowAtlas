{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We need to convert the file type and reproject our Snow Water Equivalent data into the format accepted by Google Earth Engine.\n",
    "Google Earth Engine accepts .tifs in WGS 83 (EPSG: 4326). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists, join\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import re\n",
    "import subprocess\n",
    "import urllib\n",
    "import webbrowser\n",
    "import xarray as xr\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE Grids Map Projection to WGS84\n",
    "\n",
    "The GlobSnow data product is produced by the European Space Agency as a HDF4 file. Plotting the SWE from the original file using xarray shows a Northern Hemisphere, Lambert Azimuthal projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'C:\\\\Users\\\\vicki\\\\ghw2019_SnowAtlas\\\\data\\\\01\\\\GlobSnow_SWE_L3B_monthly_201601_v2.0.hdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: [<function _open_netcdf4_group at 0x00000259FEE4E1E0>, ('C:\\\\Users\\\\vicki\\\\ghw2019_SnowAtlas\\\\data\\\\01\\\\GlobSnow_SWE_L3B_monthly_201601_v2.0.hdf', CombinedLock([<SerializableLock: eef40ba9-be2a-47d3-897f-9b7f3762e2b0>, <SerializableLock: 393d594f-5988-4626-a1d3-b1032e8b3d78>])), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('group', None), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e0882c442622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Take a look at one of the .hdf files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/01/GlobSnow_SWE_L3B_monthly_201601_v2.0.hdf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswe_average\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs, use_cftime)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'netcdf4'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             store = backends.NetCDF4DataStore.open(\n\u001b[1;32m--> 363\u001b[1;33m                 filename_or_obj, group=group, lock=lock, **backend_kwargs)\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'scipy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mstore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScipyDataStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mbackend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    350\u001b[0m             kwargs=dict(group=group, clobber=clobber, diskless=diskless,\n\u001b[0;32m    351\u001b[0m                         persist=persist, format=format))\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, manager, lock, autoclose)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xarray\\backends\\netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[1;34m(filename, lock, mode, group, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'C:\\\\Users\\\\vicki\\\\ghw2019_SnowAtlas\\\\data\\\\01\\\\GlobSnow_SWE_L3B_monthly_201601_v2.0.hdf'"
     ]
    }
   ],
   "source": [
    "# Take a look at one of the .hdf files \n",
    "f = \"../data/01/GlobSnow_SWE_L3B_monthly_201601_v2.0.hdf\"\n",
    "data = xr.open_dataset(f)\n",
    "data.swe_average.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to reproject this to WGS84. Unfortunately, if we look at the xarray dataset we see there are no latitude, longitude data variables. The metadata concerning the projection is missing. Without this projection information, GDAL does not know how to map the pixel coordinates to latitude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically obtain projection data\n",
    "\n",
    "The parameters of the grid can be found at the [National Snow & Ice Data Center](https://nsidc.org/ease/ease-grid-projection-gt). We wrote a class to obtain the projection data from the data provided on the NSIDC website. The EPSG and map coordinates of pixel corners must be manually provided to the GDAL translate utility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE_Parameters(object):\n",
    "    \n",
    "    def __init__(self, version=1, hemisphere='Northern'):\n",
    "\n",
    "        '''\n",
    "        Retrieve CRS information from URL.\n",
    "        \n",
    "        :param version: The version number (1 or 2) of the grid\n",
    "        :param hemisphere: The hemisphere of interest (Northern or Southern)\n",
    "        '''\n",
    "        # Set the grid name using the supplied version number\n",
    "        self.version = version\n",
    "        grid_name = {\n",
    "            1: 'EASE-Grid',\n",
    "            2: 'EASE-Grid 2.0'\n",
    "        }\n",
    "        # Validate user input\n",
    "        try:\n",
    "            self.grid = grid_name[self.version]\n",
    "        except KeyError:\n",
    "            raise UserWarning('You must choose between version 1 and version 2.')\n",
    "        \n",
    "        # Validate the hemisphere input\n",
    "        self.hemisphere = hemisphere\n",
    "        if self.hemisphere.lower() not in ['northern', 'southern']:\n",
    "            raise UserWarning('You must chose between the Northern and Southern hemisphere.')\n",
    " \n",
    "        self.url = \"https://nsidc.org/ease/ease-grid-projection-gt\"\n",
    "        self._table_number = -2\n",
    "        \n",
    "    @property\n",
    "    def table_number(self):\n",
    "        return self._table_number\n",
    "    \n",
    "    @table_number.setter\n",
    "    def table_number(self, n):\n",
    "        self._table_number = n\n",
    "\n",
    "    @property\n",
    "    def table(self):\n",
    "        '''\n",
    "        Return the dataframe for the grid and resolution of interest.\n",
    "        Rather than pass resolution as a parameter, select the table number from\n",
    "        \n",
    "        '''\n",
    "        tables = pd.read_html(self.url)\n",
    "        num_tables = len(tables) -1\n",
    "        if abs(self._table_number) > num_tables:\n",
    "            raise UserWarning(f'There are only {num_tables} tables, please select a valid table number.')\n",
    "        \n",
    "        df = tables[self._table_number]\n",
    "        # The Grid Name column name varies over tables, use column number\n",
    "        return df[(df[df.columns[0]]==self.grid) & (df['Projection'].str.contains(self.hemisphere, case=False))]\n",
    "    \n",
    "    @property\n",
    "    def epsg(self):\n",
    "        '''\n",
    "        Return the EPSG code corresponding to the CRS. \n",
    "        '''\n",
    "        proj = self.table['Projection'].item()\n",
    "        pattern = re.compile('EPSG: \\d{4}')\n",
    "        epsg = re.search(pattern, proj)[0]\n",
    "        return epsg.replace(' ', '')\n",
    "    \n",
    "    @property\n",
    "    def num_cols(self):\n",
    "        '''\n",
    "        Return the number of columns in the grid.\n",
    "        '''\n",
    "        return self.table['Number of Columns'].item()\n",
    "\n",
    "    @property\n",
    "    def num_rows(self):\n",
    "        '''\n",
    "        Return the number of rows in teh grid.\n",
    "        '''\n",
    "        return self.table['Number of Rows'].item()\n",
    "    \n",
    "    @property\n",
    "    def grid_size(self):\n",
    "        '''\n",
    "        Grid cell area is reported as a string {number} {unit} x {number} {unit}.\n",
    "        Parse the string and take the first number as the size\n",
    "        '''\n",
    "        return float(self.table['Grid Cell Area'].item().split(' ')[0].replace(',',''))\n",
    "    \n",
    "    # TODO: Debug key error \n",
    "    @property\n",
    "    def ulx(self):\n",
    "        try:\n",
    "            return self.table['x-axis map coordinate of the outer edge of the upper-left pixel'].item()\n",
    "        except KeyError:\n",
    "            return self.table[self.table.columns[-2]].item()\n",
    "                \n",
    "    @property\n",
    "    def uly(self):\n",
    "        try:\n",
    "            return self.table['y-axis map coordinate of the outer edge of the upper-left pixel'].item()\n",
    "        except KeyError:\n",
    "            return self.table[self.table.columns[-1]].item()\n",
    "    \n",
    "    @property\n",
    "    def lrx(self):\n",
    "        '''\n",
    "        Return the lower right x coord by multiplying the grid size by the number of columns.\n",
    "        '''\n",
    "        return self.ulx + self.grid_size*self.num_cols\n",
    "    \n",
    "    @property \n",
    "    def lry(self):\n",
    "        '''\n",
    "        Return the lower right x coord by multiplying the grid size by the number of rows.\n",
    "        '''\n",
    "        return self.uly - self.grid_size*self.num_rows\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HDF file to TIF file\n",
    "When GDAL is given the projection information, it can be used to convert to a GeoTiff file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_globsnow(directory):\n",
    "    '''\n",
    "    Download all available data.\n",
    "    '''\n",
    "    for year in range(1997, 2017):\n",
    "        for month in range(1, 13):\n",
    "            mon = \"{0:0=2d}\".format(month)\n",
    "            filename = f\"GlobSnow_SWE_L3B_monthly_{year}{mon}_v2.0.hdf\"\n",
    "            folder = join(directory, mon)\n",
    "            if not exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            if not exists(join(folder, filename)):\n",
    "                url = f\"http://www.globsnow.info/swe/archive_v2.0/{year}/L3B_monthly_SWE_HDF/{filename}\"\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, join(folder, filename))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# File conversion function \n",
    "def translate_globsnow(directory, month):\n",
    "    '''\n",
    "    Add meta data to the raw file and convert to gdal virtual format.\n",
    "    '''\n",
    "    # Obtain CRS information\n",
    "    param = EASE_Parameters()\n",
    "    epsg = param.epsg\n",
    "    ulx, uly, lrx, lry = param.ulx, param.uly, param.lrx, param.lry\n",
    "    \n",
    "    # Find files that match this pattern\n",
    "    pattern = re.compile(\"(GlobSnow_SWE_L3B_monthly_(\\d{4})(\\d{2})_v2.0.hdf)\")\n",
    "    folder = join(directory, month)\n",
    "    for f in glob(join(folder, '*.hdf')):\n",
    "        match = re.search(pattern, f)\n",
    "        if match is not None:\n",
    "            # Create output filename from input filename\n",
    "            input_file, year, month = match.groups()\n",
    "            output_file = f\"GlobSnow_SWE_Average_{year}_{month}.vrt\"\n",
    "            bash_cmd = (\n",
    "                f\"gdal_translate -of VRT -a_nodata -1 \"\n",
    "                f\"-a_srs {epsg} -a_ullr {ulx} {uly} {lrx} {lry} \"\n",
    "                f\"HDF4_SDS:UNKNOWN:\\\"{input_file}\\\":0 {output_file}\"\n",
    "            )\n",
    "            subprocess.Popen(bash_cmd, cwd=folder, shell=True, executable='/bin/bash')\n",
    "\n",
    "def convert_globsnow(directory, month):\n",
    "    '''\n",
    "    Convert .vrt to a .tif file.\n",
    "    '''\n",
    "    pattern = re.compile(\"GlobSnow_SWE_Average_\\d{4}_\\d{2}.vrt\")\n",
    "    folder = join(directory, month)\n",
    "    for f in glob(join(folder, '*.vrt')):\n",
    "        match = re.search(pattern, f)\n",
    "        if match is not None:\n",
    "            input_file = match[0]\n",
    "            output_file = input_file.replace('.vrt', '.tif')\n",
    "            bash_cmd = (\n",
    "                f\"gdalwarp -of GTiff \"\n",
    "                f\"-t_srs EPSG:4326 -r cubic \"\n",
    "                f\"{input_file} {output_file}\"\n",
    "            )\n",
    "            subprocess.Popen(bash_cmd, cwd=folder, shell=True, executable='/bin/bash')\n",
    "     \n",
    "    \n",
    "def plot(filename):\n",
    "    '''\n",
    "    Plot image.\n",
    "    '''\n",
    "    im = Image.open(filename)\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replotting the data we can see it has been reprojected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code \n",
    "# download_globsnow(\"../data\")\n",
    "# for month in [\"{0:0=2d}\".format(m) for m in range(1,13)]:\n",
    "#     translate_globsnow(\"../data\", month)\n",
    "#     convert_globsnow(\"../data\", month)\n",
    "plot(\"../data/01/GlobSnow_SWE_Average_2016_01.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Earth Engine\n",
    "\n",
    "Import tif to Google Earth Engine and include the data as a parameter in classification. [Open GEE Code Editor]('https://code.earthengine.google.com/0eb3b00f6829f6e73f53dea2fe325761')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://code.earthengine.google.com/0eb3b00f6829f6e73f53dea2fe325761'\n",
    "webbrowser.open_new(url)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
